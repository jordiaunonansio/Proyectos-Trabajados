{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_CRNN_seq import CRNN, SimpleCNN\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CTCLoss\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CTCLoss\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "abc = {\n",
    "    \"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"H\": 7, \"I\": 8, \"J\": 9, \"K\": 10, \"L\": 11, \"M\": 12, \"N\": 13, \"O\": 14, \"P\": 15, \"Q\": 16, \"R\": 17, \"S\": 18, \"T\": 19, \"U\": 20, \"V\": 21, \"W\": 22, \"X\": 23, \"Y\": 24, \"Z\": 25, \" \": 26\n",
    "}\n",
    "\n",
    "inverted_abc = {v: k for k, v in abc.items()}\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=ToTensor(), target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.imgs = []\n",
    "        for dirpath, dirnames, filenames in os.walk(self.root_dir):\n",
    "            for dirname in dirnames:\n",
    "                filenames = sorted(os.listdir(os.path.join(dirpath, dirname)))\n",
    "                for filename in filenames:\n",
    "                    path = os.path.join(dirpath, dirname, filename)\n",
    "                    value_before_dot = filename.rsplit('.', 1)[0][-1]\n",
    "                    target = self.get_target(dirname, value_before_dot)\n",
    "                    length = len(filename)\n",
    "                    self.imgs.append((path, target, length))\n",
    "\n",
    "\n",
    "    def get_target(self, path, index):\n",
    "        # Extract the specific character from the path and convert it to your target\n",
    "        # This is just an example, adjust this to fit your needs\n",
    "        target_char = path[int(index)]  \n",
    "        if target_char in \"- '\":\n",
    "            target= 26\n",
    "        else:\n",
    "            target = abc[target_char]  # Convert the character to an integer\n",
    "        return target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target, length = self.imgs[index]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target, length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Convert to grayscale\n",
    "    transforms.Resize((28, 28)),  # Resize to 32x32\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "# Create the datasets\n",
    "val_dataset = CustomDataset(root_dir='/home/xnmaster/XNAPproject-grup_08/words_segmented_test', transform=transform)\n",
    "\n",
    "# Create the data loaders\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_CRNN_seq import CRNN, SimpleCNN\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CTCLoss\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "abc = {\n",
    "    \"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"H\": 7, \"I\": 8, \"J\": 9, \"K\": 10, \"L\": 11, \"M\": 12, \"N\": 13, \"O\": 14, \"P\": 15, \"Q\": 16, \"R\": 17, \"S\": 18, \"T\": 19, \"U\": 20, \"V\": 21, \"W\": 22, \"X\": 23, \"Y\": 24, \"Z\": 25, \" \": 26\n",
    "}\n",
    "\n",
    "inverted_abc = {v: k for k, v in abc.items()}\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = SimpleCNN()\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('/home/xnmaster/XNAPproject-grup_08/model_CNN/results/dataset2/trained_model.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# Create the CRNN model\n",
    "crnn_model = CRNN(cnn_model, rnn_hidden_size=256, num_classes=27)  # 26 letters + 1 for blank\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('/home/xnmaster/XNAPproject-grup_08/model_CRNN/crnn_model.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "\n",
    "# Load the state dictionary\n",
    "crnn_model.load_state_dict(state_dict)\n",
    "# Set the model to evaluation mode\n",
    "crnn_model.eval()\n",
    "\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "loss_function = CTCLoss(blank=26, zero_infinity=True)  # Assuming blank label is 26\n",
    "optimizer = optim.Adam(crnn_model.parameters())\n",
    "\n",
    "\n",
    "# testing loop\n",
    "\n",
    "lib_images = []\n",
    "lib_labels = []\n",
    "lib_lengths = []\n",
    "\n",
    "word_images = []\n",
    "word_labels = []\n",
    "word_lengths = []\n",
    "lns = []\n",
    "for i, (images, labels, lengths) in enumerate(val_loader):\n",
    "    lns.append(lengths)\n",
    "    # If the current word is finished, process the batch and start a new one\n",
    "    if i > 0 and lns[i-1] != lns[i]:\n",
    "        word_images = torch.stack(word_images)\n",
    "        word_labels = torch.stack(word_labels)\n",
    "        word_lengths = torch.stack(word_lengths)\n",
    "        # Move the images and labels to the GPU if available\n",
    "        lib_images.append(word_images)\n",
    "        lib_labels.append(word_labels) \n",
    "        lib_lengths.append(word_lengths)\n",
    "        \n",
    "        word_images = []\n",
    "        word_labels = []\n",
    "        word_lengths = []\n",
    "    word_images.append(images)\n",
    "    word_labels.append(labels)\n",
    "    word_lengths.append(lengths)\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "    # process the batch\n",
    "lib_images = lib_images[1:]\n",
    "lib_labels = lib_labels[1:]\n",
    "lib_lengths = lib_lengths[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -4.3991090329362885e-05, Accuracy: 0.00013979822456254806\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for word_images, word_labels, word_lengths in zip(lib_images, lib_labels, lib_lengths):\n",
    "        word_images = word_images.to(device)\n",
    "        word_labels = word_labels.to(device)\n",
    "        word_lengths = word_lengths.to(device)\n",
    "        outputs = crnn_model(word_images)\n",
    "        outputs = outputs.transpose(0, 1)\n",
    "        input_lengths = torch.tensor([outputs.size(0)], dtype=torch.long)        \n",
    "        target_lengths = torch.tensor([word_labels.size(0)], dtype=torch.long)\n",
    "        word_labels = word_labels.unsqueeze(0)\n",
    "        word_labels = word_labels.view(-1)\n",
    "        loss = loss_function(outputs, word_labels, input_lengths, target_lengths)\n",
    "        test_loss += loss.item()\n",
    "        pred = outputs.argmax(dim=2)\n",
    "        pred_str = ''\n",
    "        for l in pred:\n",
    "            pred_str += inverted_abc[l.item()]\n",
    "        label_str = ''\n",
    "        for l in word_labels:\n",
    "            label_str += inverted_abc[l.item()]\n",
    "        if pred_str == label_str:\n",
    "            correct += 1\n",
    "test_loss /= len(val_loader)\n",
    "accuracy = correct / len(val_loader)\n",
    "print(f'Loss: {test_loss}, Accuracy: {accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'A',\n",
       " 1: 'B',\n",
       " 2: 'C',\n",
       " 3: 'D',\n",
       " 4: 'E',\n",
       " 5: 'F',\n",
       " 6: 'G',\n",
       " 7: 'H',\n",
       " 8: 'I',\n",
       " 9: 'J',\n",
       " 10: 'K',\n",
       " 11: 'L',\n",
       " 12: 'M',\n",
       " 13: 'N',\n",
       " 14: 'O',\n",
       " 15: 'P',\n",
       " 16: 'Q',\n",
       " 17: 'R',\n",
       " 18: 'S',\n",
       " 19: 'T',\n",
       " 20: 'U',\n",
       " 21: 'V',\n",
       " 22: 'W',\n",
       " 23: 'X',\n",
       " 24: 'Y',\n",
       " 25: 'Z',\n",
       " 26: ' '}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
